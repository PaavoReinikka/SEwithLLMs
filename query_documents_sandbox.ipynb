{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a893c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.llms import Ollama\n",
    "from langchain_ollama import OllamaLLM as Ollama, OllamaEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# for text splitting\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "# import CharacterTextSplitter\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "# for embeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f631e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b536be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c1e4c",
   "metadata": {},
   "source": [
    "### Setting up the embeddings\n",
    "\n",
    "For augmenting our query with context information effectively, we can \n",
    "1. embed the source material in a semantically meaningful vector space -- i.e., represent blocks of text with numerical vectors.\n",
    "2. then do the same for the query.\n",
    "3. compare the query's vector representation agains all the vectors of the source material.\n",
    "4. return the block of text that corresponds to most similar vector.\n",
    "\n",
    "A practical issue with embedding using Ollama is that we need a different model for the embedding and the llm. If we use our local Ollama server for both, it has to load the 2 different models alternately. If both models are sufficiently small (compared to system memory), one way to deal with this is by running the embedding model in a separate container. Or we can use a Huggingface model for embedding and ollama for llm. There are still issues with running multiple models in parallel locally. \n",
    "\n",
    "Ideally we could use a remote endpoint for the llm and run the embedding locally -- embedding large text corpora over http connection is not a good idea. However, this would mean we'd have to purchase compute from some api service. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3816e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the embeddings (OPTION 1, simplest)\n",
    "embedding_model=\"all-minilm\"#\"mxbai-embed-small\"\n",
    "embeddings = OllamaEmbeddings(model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4a6524a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the embeddings (OPTION 2, in a container)\n",
    "# assuming we have a local Ollama container running on port 11435 (see launch_ollama.sh)\n",
    "embedding_model=\"all-minilm\"#\"mxbai-embed-small\"\n",
    "embeddings = OllamaEmbeddings(base_url=\"http://localhost:11435\",\n",
    "                              model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d8af917f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the embeddings (OPTION 3, locally using HuggingFace)\n",
    "embedding_model =\"thenlper/gte-small\"# \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {'device': device}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "61e5bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the llm (assumes we have Ollama running in a container on default port 11434)\n",
    "llm = Ollama(model='llama3.2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0762e4d",
   "metadata": {},
   "source": [
    "### Reading the document and loading it into a vector store\n",
    "\n",
    "* loader loads the document, then splits it into chunks. I am using defaults here, but we could also get more control over the text splitting.\n",
    "* We make a vector store from the documents, and index it for fast similarity search based on embedding vectors.\n",
    "* finally we make a langchain `retriever` that can be directly invoked with queries -- it returns the most similar text block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2821813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load the context information and create a retriever\n",
    "path = \"./documents/\"\n",
    "#loader = TextLoader(path + \"sample.txt\")\n",
    "loader = PyPDFLoader(path + \"benjamini_yekutieli_fdr.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "177cb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defaults\n",
    "pages = loader.load_and_split()\n",
    "store = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)\n",
    "retriever = store.as_retriever(search_kwargs={\"k\": 1}) # number of chunks to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "beb41491",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(\n",
    "    min_chunk_size=500,\n",
    "    embeddings=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b4c5db82",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=20,\n",
    "    length_function=len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f1f56ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", chunk_size=500, chunk_overlap=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a14da579",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loader.load_and_split(text_splitter=text_splitter)\n",
    "store = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)\n",
    "retriever = store.as_retriever(search_kwargs={\"k\": 1}) # number of chunks to return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "98bd0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create the prompt template\n",
    "template = \"\"\"\n",
    "Answer the question based on the context provided if possible.\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2bd1bfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(template)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d39343",
   "metadata": {},
   "source": [
    "### Chaining the the operations\n",
    "\n",
    "See: https://python.langchain.com/docs/versions/migrating_chains/\n",
    "\n",
    "Chains are a useful abstraction for a pipeline of common operations that most LLM applications need. \n",
    "\n",
    "Here we are:\n",
    "\n",
    "1. invoking the retriever with our query, and it's returning most relevant context (with metadata as well, which we don't use here)\n",
    "2. formatting the retrieved context (keeping just the text)\n",
    "3. mapping the query and context into the prompt template.\n",
    "4. passing the formatted prompt to llm\n",
    "5. formatting the output with `StrOutputParser()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "270a2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=retriever.invoke(\"idea of fdr control\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "481cf17d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Acrobat Distiller 4.05 for Windows',\n",
       " 'creator': 'DVIPSONE (32) 2.1.3 http://www.YandY.com',\n",
       " 'creationdate': 'D:20011203123023',\n",
       " 'title': 'FRANJIMSAOS\\x029-4AOS260',\n",
       " 'author': 'Dr. Mirko Janc (Tech Typeset) 427 1999 Feb 15 15:33:29',\n",
       " 'subject': 'TeX output 2001.11.15:0819',\n",
       " 'source': 'benjamini_yekutieli_fdr.pdf',\n",
       " 'total_pages': 24,\n",
       " 'page': 4,\n",
       " 'page_label': '5'}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "763fe2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CONTROLLING THE FDR UNDER DEPENDENCY 1169\\nIn Section 2 we discuss in more detail the FDR criterion, the historical\\nbackground of the procedure and available results and review the relevant\\nnotions of positive dependency. This section can be consulted as needed. In\\nSection 3 we outline some important problems where it is natural to assume\\nthat the conditions of Theorem 1.2 hold. In Section 4 we prove the theorem. In the course of the proof we provide an explicit expression for the FDR, from\\nwhich many more new properties can be derived, both for the independent and\\nthe dependent cases. Thus issues such as discrete test statistics, composite\\nnull hypotheses, general step-up procedures and general dependency can be\\naddressed. This is done in Section 5. In particular we prove there the following\\ntheorem. Theorem 1.3. When the Benjamini Hochberg procedure is conducted with\\nq//lparenOSCASB∑ m\\ni=1\\n1\\ni/rparenOSCASBtaking the place of qin (1), it always controls the FDR at level less\\nthan or equal to m0\\nmq. As can be seen from the above summary, the results of this article greatly\\nincrease the range of problems for which a powerful procedure with proven\\nFDR control can be offered. 2.'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_docs(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e32d3416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt.invoke({\"context\": format_docs(res), \"question\": \"idea of fdr control?\"}).text.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "04a8d3fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Acrobat Distiller 4.05 for Windows', 'creator': 'DVIPSONE (32) 2.1.3 http://www.YandY.com', 'creationdate': 'D:20011203123023', 'title': 'FRANJIMSAOS\\x029-4AOS260', 'author': 'Dr. Mirko Janc (Tech Typeset) 427 1999 Feb 15 15:33:29', 'subject': 'TeX output 2001.11.15:0819', 'source': 'benjamini_yekutieli_fdr.pdf', 'total_pages': 24, 'page': 4, 'page_label': '5'}, page_content='CONTROLLING THE FDR UNDER DEPENDENCY 1169\\nIn Section 2 we discuss in more detail the FDR criterion, the historical\\nbackground of the procedure and available results and review the relevant\\nnotions of positive dependency. This section can be consulted as needed. In\\nSection 3 we outline some important problems where it is natural to assume\\nthat the conditions of Theorem 1.2 hold. In Section 4 we prove the theorem. In the course of the proof we provide an explicit expression for the FDR, from\\nwhich many more new properties can be derived, both for the independent and\\nthe dependent cases. Thus issues such as discrete test statistics, composite\\nnull hypotheses, general step-up procedures and general dependency can be\\naddressed. This is done in Section 5. In particular we prove there the following\\ntheorem. Theorem 1.3. When the Benjamini Hochberg procedure is conducted with\\nq//lparenOSCASB∑ m\\ni=1\\n1\\ni/rparenOSCASBtaking the place of qin (1), it always controls the FDR at level less\\nthan or equal to m0\\nmq. As can be seen from the above summary, the results of this article greatly\\nincrease the range of problems for which a powerful procedure with proven\\nFDR control can be offered. 2.')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What is the FDR?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2e3dd154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='\\nAnswer the question based on the context provided if possible.\\nContext: CONTROLLING THE FDR UNDER DEPENDENCY 1169\\nIn Section 2 we discuss in more detail the FDR criterion, the historical\\nbackground of the procedure and available results and review the relevant\\nnotions of positive dependency. This section can be consulted as needed. In\\nSection 3 we outline some important problems where it is natural to assume\\nthat the conditions of Theorem 1.2 hold. In Section 4 we prove the theorem. In the course of the proof we provide an explicit expression for the FDR, from\\nwhich many more new properties can be derived, both for the independent and\\nthe dependent cases. Thus issues such as discrete test statistics, composite\\nnull hypotheses, general step-up procedures and general dependency can be\\naddressed. This is done in Section 5. In particular we prove there the following\\ntheorem. Theorem 1.3. When the Benjamini Hochberg procedure is conducted with\\nq//lparenOSCASB∑ m\\ni=1\\n1\\ni/rparenOSCASBtaking the place of qin (1), it always controls the FDR at level less\\nthan or equal to m0\\nmq. As can be seen from the above summary, the results of this article greatly\\nincrease the range of problems for which a powerful procedure with proven\\nFDR control can be offered. 2.\\nQuestion: What is the FDR?\\n')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    {\n",
    "        'context': retriever | format_docs,\n",
    "        'question': RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    ").invoke(\"What is the FDR?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aef270",
   "metadata": {},
   "source": [
    "**Now chaining all the operations into one chai:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f506cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        'context': retriever | format_docs,\n",
    "        'question': RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser() # or JsonOutputParser() etc. for formatted output depending on the LLM used and the need\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5c2efb",
   "metadata": {},
   "source": [
    "**invoking the chain with query/question:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f0c9fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain.invoke(\n",
    "    \"implement fdr control in multiple testing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf9c3a0",
   "metadata": {},
   "source": [
    "**and viewing the output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8279d68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Based on the context provided, to implement FDR control in multiple testing, you would need to follow these general steps:',\n",
       " '',\n",
       " '1. **Determine the number of independent tests**: Identify the total number of hypotheses being tested (R).',\n",
       " '',\n",
       " '2. **Compute the test statistic**: For each test, calculate a test statistic that determines whether a hypothesis is rejected.',\n",
       " '',\n",
       " '3. **Calculate the proportion of true null hypotheses rejected**: Calculate V/R, where V is the number of true null hypotheses rejected and R is the total number of hypotheses rejected.',\n",
       " '',\n",
       " '4. **Calculate the FDR value**: Calculate the FDR (False Discovery Rate) as E[Q], where Q = V/R if R > 0, or 0 otherwise.',\n",
       " '',\n",
       " '5. **Choose a significance level q**: Choose an FDR level q that you want to control, typically at a conventional level for α (e.g., 0.05).',\n",
       " '',\n",
       " '6. **Implement the step-up procedure**: Implement the step-up procedure, which rejects all hypotheses with a p-value less than or equal to the critical value determined by the FDR level q.',\n",
       " '',\n",
       " '7. **Repeat as necessary**: Repeat steps 2-6 for each family of tests, if multiple families are being tested simultaneously.',\n",
       " '',\n",
       " 'Note that the specific implementation details may vary depending on the type of test statistics used (e.g., discrete test statistics), composite null hypotheses, general step-up procedures, or other factors discussed in Section 5.']"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aa4f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

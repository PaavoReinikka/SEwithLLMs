{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "This is just a prototyping notebook for tool calling with langchain and ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_ollama import ChatOllama, OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "import logging\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from tool_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    \"llama3.2\",\n",
    "    model_provider=\"ollama\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ChatOllama(model=\"llama3.2\", format=\"json\", temperature=0)\n",
    "model_with_tools = model.bind_tools(tools=get_tools())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'fetch_current_electricity_price', 'args': {}, 'id': 'c2241cf3-a86a-4a5c-9935-d0d811b3f1ea', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the current electricity price in Finland?\")]\n",
    "response = model_with_tools.invoke(messages)\n",
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'fetch_current_electricity_price',\n",
       "  'args': {},\n",
       "  'id': 'c2241cf3-a86a-4a5c-9935-d0d811b3f1ea',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"\"\"You are a helpful assistant and you can call functions to get information.\n",
    "    You can call multiple functions if necessary.\n",
    "                  \"\"\"\n",
    "                  ),\n",
    "    HumanMessage(content=\"What is the current date? Also what is current time?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant and you can call functions to get information.\\n    You can call multiple functions if necessary.\\n                  ', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the current date? Also what is current time?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-27T19:42:01.314577523Z', 'done': True, 'done_reason': 'stop', 'total_duration': 909250109, 'load_duration': 17007963, 'prompt_eval_count': 895, 'prompt_eval_duration': 140000000, 'eval_count': 28, 'eval_duration': 316000000, 'model_name': 'llama3.2'}, id='run-3020a388-52c5-4362-be8f-3be88dce5598-0', tool_calls=[{'name': 'get_current_date', 'args': {}, 'id': '000ad38d-4401-4950-ad42-1cb0989113b8', 'type': 'tool_call'}, {'name': 'get_current_time', 'args': {}, 'id': '63a4fc11-78e5-4c55-9ba6-1b907c3d4d90', 'type': 'tool_call'}], usage_metadata={'input_tokens': 895, 'output_tokens': 28, 'total_tokens': 923}),\n",
       " ToolMessage(content='2025-04-27', name='get_current_date', tool_call_id='000ad38d-4401-4950-ad42-1cb0989113b8'),\n",
       " ToolMessage(content='22:42', name='get_current_time', tool_call_id='63a4fc11-78e5-4c55-9ba6-1b907c3d4d90')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_message = model_with_tools.invoke(messages)\n",
    "messages.append(ai_message)\n",
    "for tool_call in ai_message.tool_calls:\n",
    "    selected_tool = tool_map[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The current date is April 27, 2025. The current time is 10:42 PM.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-04-27T19:42:05.661919914Z', 'done': True, 'done_reason': 'stop', 'total_duration': 212615509, 'load_duration': 15976174, 'prompt_eval_count': 140, 'prompt_eval_duration': 4000000, 'eval_count': 23, 'eval_duration': 191000000, 'model_name': 'llama3.2'} id='run-6810b470-cf5c-4aab-985c-92cc05e4c1f7-0' usage_metadata={'input_tokens': 140, 'output_tokens': 23, 'total_tokens': 163}\n"
     ]
    }
   ],
   "source": [
    "response = model_with_tools.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are a helpful assistant and you can call functions to get information.\n",
    "If you need to use the outputs of functions as inputs to other functions, you can do that\n",
    "also. You can call multiple functions if necessary. Explain what you are doing in the process.\n",
    "You can also ask clarifying questions if you need more information.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", system_message),\n",
    "        (\"human\", \"{user_input}\"),        \n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.prompts.chat.ChatPromptTemplate"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-28T09:00:57.376071878Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1970160616, 'load_duration': 16041664, 'prompt_eval_count': 934, 'prompt_eval_duration': 74000000, 'eval_count': 28, 'eval_duration': 1877000000, 'model_name': 'llama3.2'}, id='run-89a48ac3-3f1b-446c-a273-ff15f4bbc1d2-0', tool_calls=[{'name': 'get_current_date', 'args': {}, 'id': '7d08fb8e-a693-42c8-a9a9-27cfe9f496cb', 'type': 'tool_call'}, {'name': 'get_current_time', 'args': {}, 'id': 'f4312f22-828e-4838-92c5-37ebc839b33f', 'type': 'tool_call'}], usage_metadata={'input_tokens': 934, 'output_tokens': 28, 'total_tokens': 962})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_tools.invoke(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='\\nYou are a helpful assistant and you can call functions to get information.\\nIf you need to use the outputs of functions as inputs to other functions, you can do that\\nalso. You can call multiple functions if necessary. Explain what you are doing in the process.\\nYou can also ask clarifying questions if you need more information.\\n', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the current date? Also what is current time?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=prompt.invoke({\"user_input\": \"What is the current date? Also what is current time?\"})\n",
    "res.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-28T09:31:55.740898279Z', 'done': True, 'done_reason': 'stop', 'total_duration': 20397608069, 'load_duration': 15226177, 'prompt_eval_count': 937, 'prompt_eval_duration': 18497000000, 'eval_count': 28, 'eval_duration': 1883000000, 'model_name': 'llama3.2'}, id='run-4d96106f-2720-4087-a375-9950556b8e13-0', tool_calls=[{'name': 'get_current_date', 'args': {}, 'id': 'f836c174-4c8c-41fa-9c92-5537dd58b0e8', 'type': 'tool_call'}, {'name': 'get_current_time', 'args': {}, 'id': 'e8cec759-d24c-4e1b-ba06-0136dee6a56b', 'type': 'tool_call'}], usage_metadata={'input_tokens': 937, 'output_tokens': 28, 'total_tokens': 965})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res=(prompt | model_with_tools).invoke({\"user_input\": \"What is the current date? Also what is current time?\"})\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_current_date',\n",
       "  'args': {},\n",
       "  'id': 'f836c174-4c8c-41fa-9c92-5537dd58b0e8',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'get_current_time',\n",
       "  'args': {},\n",
       "  'id': 'e8cec759-d24c-4e1b-ba06-0136dee6a56b',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "# Set up the function to call the model\n",
    "def call_model(input_text, prompt, verbose=False):\n",
    "    '''\n",
    "    Calls the AI model with the provided input text and returns the response.\n",
    "\n",
    "    Args:\n",
    "        prompt (ChatPromptTemplate): The prompt template to use for the model.\n",
    "        input_text (str): The input text to send to the AI model.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the AI model.\n",
    "        prompt (ChatPromptTemplate): The updated prompt with the model response.\n",
    "    '''\n",
    "\n",
    "    chain = prompt | model_with_tools\n",
    "    try:\n",
    "        # Call the model with the input text\n",
    "        response = chain.invoke({\"user_input\": input_text})\n",
    "        #prompt.messages.append(response)\n",
    "        if verbose: logger.info(f\"Model response: {response}\")\n",
    "        \n",
    "        # Check if the response contains tool calls\n",
    "        if response.tool_calls:\n",
    "            for tool_call in response.tool_calls:\n",
    "                selected_tool = tool_map[tool_call[\"name\"].lower()]\n",
    "                tool_msg = selected_tool.invoke(tool_call)\n",
    "                if verbose: logger.info(f\"Tool response: {tool_msg}\")\n",
    "                # Append the tool response to the messages\n",
    "                prompt.messages.append(tool_msg)\n",
    "            # call the model again with the updated messages\n",
    "            return call_model(input_text, prompt)\n",
    "        \n",
    "        if verbose: logger.info(\"No tool calls in the response.\")\n",
    "        prompt.messages.append(response)\n",
    "        return response, prompt\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calling model: {e}\")\n",
    "        return None, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: content='To find the value of x and then divide it by 3, we can follow these steps:\\n\\n1. Calculate the value of x: x = 24 - 18\\nx = 6\\n\\n2. Divide x by 3: 6 / 3\\n6 / 3 = 2.0\\n\\nTherefore, x divided by 3 is 2.0.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-04-28T09:30:16.820614534Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5022938177, 'load_duration': 15412378, 'prompt_eval_count': 148, 'prompt_eval_duration': 470000000, 'eval_count': 79, 'eval_duration': 4535000000, 'model_name': 'llama3.2'} id='run-e5c5b85e-6519-4335-8f01-e023a40f021b-0' usage_metadata={'input_tokens': 148, 'output_tokens': 79, 'total_tokens': 227}\n"
     ]
    }
   ],
   "source": [
    "input_text = \"if x = 24-18, what is x divided by 3?\"\n",
    "response, prompt = call_model(input_text, prompt=prompt)\n",
    "if response:\n",
    "    print(f\"Response: {response}\")\n",
    "else:\n",
    "    print(\"Failed to get a response from the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To find the value of x and then divide it by 3, we can follow these steps:',\n",
       " '',\n",
       " '1. Calculate the value of x: x = 24 - 18',\n",
       " 'x = 6',\n",
       " '',\n",
       " '2. Divide x by 3: 6 / 3',\n",
       " '6 / 3 = 2.0',\n",
       " '',\n",
       " 'Therefore, x divided by 3 is 2.0.']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

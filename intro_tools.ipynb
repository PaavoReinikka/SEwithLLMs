{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool calling\n",
    "\n",
    "This is just a prototyping notebook for tool calling with langchain and ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_experimental.llms.ollama_functions import OllamaFunctions\n",
    "from langchain_ollama import ChatOllama, OllamaLLM\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "import logging\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from tool_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_chat_model(\n",
    "    \"llama3.2\",\n",
    "    model_provider=\"ollama\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = ChatOllama(model=\"llama3.2\", format=\"json\", temperature=0)\n",
    "model_with_tools = model.bind_tools(tools=get_tools())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'fetch_current_electricity_price', 'args': {}, 'id': 'c2241cf3-a86a-4a5c-9935-d0d811b3f1ea', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"What is the current electricity price in Finland?\")]\n",
    "response = model_with_tools.invoke(messages)\n",
    "print(response.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'fetch_current_electricity_price',\n",
       "  'args': {},\n",
       "  'id': 'c2241cf3-a86a-4a5c-9935-d0d811b3f1ea',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"\"\"You are a helpful assistant and you can call functions to get information.\n",
    "    You can call multiple functions if necessary.\n",
    "                  \"\"\"\n",
    "                  ),\n",
    "    HumanMessage(content=\"What is the current date? Also what is current time?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant and you can call functions to get information.\\n    You can call multiple functions if necessary.\\n                  ', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What is the current date? Also what is current time?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-27T19:42:01.314577523Z', 'done': True, 'done_reason': 'stop', 'total_duration': 909250109, 'load_duration': 17007963, 'prompt_eval_count': 895, 'prompt_eval_duration': 140000000, 'eval_count': 28, 'eval_duration': 316000000, 'model_name': 'llama3.2'}, id='run-3020a388-52c5-4362-be8f-3be88dce5598-0', tool_calls=[{'name': 'get_current_date', 'args': {}, 'id': '000ad38d-4401-4950-ad42-1cb0989113b8', 'type': 'tool_call'}, {'name': 'get_current_time', 'args': {}, 'id': '63a4fc11-78e5-4c55-9ba6-1b907c3d4d90', 'type': 'tool_call'}], usage_metadata={'input_tokens': 895, 'output_tokens': 28, 'total_tokens': 923}),\n",
       " ToolMessage(content='2025-04-27', name='get_current_date', tool_call_id='000ad38d-4401-4950-ad42-1cb0989113b8'),\n",
       " ToolMessage(content='22:42', name='get_current_time', tool_call_id='63a4fc11-78e5-4c55-9ba6-1b907c3d4d90')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai_message = model_with_tools.invoke(messages)\n",
    "messages.append(ai_message)\n",
    "for tool_call in ai_message.tool_calls:\n",
    "    selected_tool = tool_map[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The current date is April 27, 2025. The current time is 10:42 PM.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-04-27T19:42:05.661919914Z', 'done': True, 'done_reason': 'stop', 'total_duration': 212615509, 'load_duration': 15976174, 'prompt_eval_count': 140, 'prompt_eval_duration': 4000000, 'eval_count': 23, 'eval_duration': 191000000, 'model_name': 'llama3.2'} id='run-6810b470-cf5c-4aab-985c-92cc05e4c1f7-0' usage_metadata={'input_tokens': 140, 'output_tokens': 23, 'total_tokens': 163}\n"
     ]
    }
   ],
   "source": [
    "response = model_with_tools.invoke(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# male a promt template\n",
    "prompt = ChatPromptTemplate(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant and you can call functions to get information.\"),\n",
    "        (\"human\", \"{user_input}\"),        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res=prompt.invoke({\"user_input\": \"What is the current date? Also what is current time?\"})\n",
    "#res.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res=(prompt | model_with_tools).invoke({\"user_input\": \"What is the current date? Also what is current time?\"})\n",
    "# (prompt | model_with_tools).invoke({\"user_input\": \"What is the current date? Also what is current time?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "# Set up the function to call the model\n",
    "def call_model(input_text, prompt, verbose=False):\n",
    "    '''\n",
    "    Calls the AI model with the provided input text and returns the response.\n",
    "\n",
    "    Args:\n",
    "        prompt (ChatPromptTemplate): The prompt template to use for the model.\n",
    "        input_text (str): The input text to send to the AI model.\n",
    "\n",
    "    Returns:\n",
    "        str: The response from the AI model.\n",
    "        prompt (ChatPromptTemplate): The updated prompt with the model response.\n",
    "    '''\n",
    "    chain = prompt | model_with_tools\n",
    "    try:\n",
    "        response = chain.invoke({\"user_input\": input_text})\n",
    "        prompt.messages.append(response)\n",
    "        if verbose: logger.info(f\"Model response: {response}\")\n",
    "        # Check if the response contains tool calls\n",
    "        if response.tool_calls:\n",
    "            for tool_call in response.tool_calls:\n",
    "                selected_tool = tool_map[tool_call[\"name\"].lower()]\n",
    "                tool_msg = selected_tool.invoke(tool_call)\n",
    "                if verbose: logger.info(f\"Tool response: {tool_msg}\")\n",
    "                # Append the tool response to the messages\n",
    "                #prompt.messages.append(response)\n",
    "                prompt.messages.append(tool_msg)\n",
    "                # call the model again with the updated messages\n",
    "            return call_model(input_text, prompt)\n",
    "        else:\n",
    "            if verbose: logger.info(\"No tool calls in the response.\")\n",
    "            # TODO: remove trace of the tool calls\n",
    "            # update the prompt with the model response\n",
    "            # and append new template with user input\n",
    "            #prompt.messages.append(response)\n",
    "            new_prompt = ChatPromptTemplate.from_messages(\n",
    "                prompt.messages + [\n",
    "                    (\"human\", \"{user_input}\"), # this will be replaced in the next call\n",
    "                ]\n",
    "            )\n",
    "        return response, new_prompt\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calling model: {e}\")\n",
    "        return None, prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: content='The current date is April 27, 2025. The current time is 9:09 PM.' additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-04-27T20:09:18.537538518Z', 'done': True, 'done_reason': 'stop', 'total_duration': 204289778, 'load_duration': 14216840, 'prompt_eval_count': 130, 'prompt_eval_duration': 3000000, 'eval_count': 23, 'eval_duration': 185000000, 'model_name': 'llama3.2'} id='run-77c84458-b472-4ca2-9abe-bb8b4181c2ee-0' usage_metadata={'input_tokens': 130, 'output_tokens': 23, 'total_tokens': 153}\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What is the current date? Also what is the current time?\"\n",
    "response, prompt = call_model(input_text, prompt=prompt)\n",
    "if response:\n",
    "    print(f\"Response: {response}\")\n",
    "else:\n",
    "    print(\"Failed to get a response from the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant and you can call functions to get information.'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-27T20:09:18.330728369Z', 'done': True, 'done_reason': 'stop', 'total_duration': 472492817, 'load_duration': 15523869, 'prompt_eval_count': 885, 'prompt_eval_duration': 154000000, 'eval_count': 28, 'eval_duration': 300000000, 'model_name': 'llama3.2'}, id='run-2b6586cf-710f-41b7-96eb-5b4a12f23732-0', tool_calls=[{'name': 'get_current_date', 'args': {}, 'id': '68cf792d-6758-4ccd-99e8-e3cc6c3dbae1', 'type': 'tool_call'}, {'name': 'get_current_time', 'args': {}, 'id': '8bd0f032-d00d-4aa5-b742-6061c876c400', 'type': 'tool_call'}], usage_metadata={'input_tokens': 885, 'output_tokens': 28, 'total_tokens': 913}),\n",
       " ToolMessage(content='2025-04-27', name='get_current_date', tool_call_id='68cf792d-6758-4ccd-99e8-e3cc6c3dbae1'),\n",
       " ToolMessage(content='23:09', name='get_current_time', tool_call_id='8bd0f032-d00d-4aa5-b742-6061c876c400'),\n",
       " AIMessage(content='The current date is April 27, 2025. The current time is 9:09 PM.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-27T20:09:18.537538518Z', 'done': True, 'done_reason': 'stop', 'total_duration': 204289778, 'load_duration': 14216840, 'prompt_eval_count': 130, 'prompt_eval_duration': 3000000, 'eval_count': 23, 'eval_duration': 185000000, 'model_name': 'llama3.2'}, id='run-77c84458-b472-4ca2-9abe-bb8b4181c2ee-0', usage_metadata={'input_tokens': 130, 'output_tokens': 23, 'total_tokens': 153}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "response, prompt = call_model(\"what is 12.34 - 337.1 using a tool?\", prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant and you can call functions to get information.'), additional_kwargs={}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-27T20:09:18.330728369Z', 'done': True, 'done_reason': 'stop', 'total_duration': 472492817, 'load_duration': 15523869, 'prompt_eval_count': 885, 'prompt_eval_duration': 154000000, 'eval_count': 28, 'eval_duration': 300000000, 'model_name': 'llama3.2'}, id='run-2b6586cf-710f-41b7-96eb-5b4a12f23732-0', tool_calls=[{'name': 'get_current_date', 'args': {}, 'id': '68cf792d-6758-4ccd-99e8-e3cc6c3dbae1', 'type': 'tool_call'}, {'name': 'get_current_time', 'args': {}, 'id': '8bd0f032-d00d-4aa5-b742-6061c876c400', 'type': 'tool_call'}], usage_metadata={'input_tokens': 885, 'output_tokens': 28, 'total_tokens': 913}),\n",
       " ToolMessage(content='2025-04-27', name='get_current_date', tool_call_id='68cf792d-6758-4ccd-99e8-e3cc6c3dbae1'),\n",
       " ToolMessage(content='23:09', name='get_current_time', tool_call_id='8bd0f032-d00d-4aa5-b742-6061c876c400'),\n",
       " AIMessage(content='The current date is April 27, 2025. The current time is 9:09 PM.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-27T20:09:18.537538518Z', 'done': True, 'done_reason': 'stop', 'total_duration': 204289778, 'load_duration': 14216840, 'prompt_eval_count': 130, 'prompt_eval_duration': 3000000, 'eval_count': 23, 'eval_duration': 185000000, 'model_name': 'llama3.2'}, id='run-77c84458-b472-4ca2-9abe-bb8b4181c2ee-0', usage_metadata={'input_tokens': 130, 'output_tokens': 23, 'total_tokens': 153}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={}),\n",
       " AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-27T20:09:32.312386896Z', 'done': True, 'done_reason': 'stop', 'total_duration': 438981755, 'load_duration': 14020871, 'prompt_eval_count': 981, 'prompt_eval_duration': 145000000, 'eval_count': 22, 'eval_duration': 272000000, 'model_name': 'llama3.2'}, id='run-c2995a99-e800-4712-b265-fb6e51e9638d-0', tool_calls=[{'name': 'subtract', 'args': {'a': '12', 'b': '337'}, 'id': '5231841f-9f45-4c59-b086-5b5e9635fe6c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 981, 'output_tokens': 22, 'total_tokens': 1003}),\n",
       " ToolMessage(content='-325', name='subtract', tool_call_id='5231841f-9f45-4c59-b086-5b5e9635fe6c'),\n",
       " AIMessage(content='The result of subtracting 337.1 from 12.34 is -325.', additional_kwargs={}, response_metadata={'model': 'llama3.2', 'created_at': '2025-04-27T20:09:32.494876065Z', 'done': True, 'done_reason': 'stop', 'total_duration': 180464134, 'load_duration': 15688270, 'prompt_eval_count': 210, 'prompt_eval_duration': 4000000, 'eval_count': 19, 'eval_duration': 158000000, 'model_name': 'llama3.2'}, id='run-36ff6929-9d22-4e4c-a82a-2de669ba0675-0', usage_metadata={'input_tokens': 210, 'output_tokens': 19, 'total_tokens': 229}),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['user_input'], input_types={}, partial_variables={}, template='{user_input}'), additional_kwargs={})]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The result of subtracting 337.1 from 12.34 is -324.66 (rounded to two decimal places).'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
